{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions and Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes contain instructions and questions for the labs portion of the \"Big Data: tools and statistics\" course. Within this document, command-line steps are presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -text /example/wordcountout/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All commands will be in a separate grey \"cell\" (as above).\n",
    "<br><br>\n",
    "Exercises will be listed as a bulleted item and italicized. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Create a new directory in your home directory called sample. Upload data.csv into the sample directory on HDFS.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be expected to achieve the following:\n",
    "<ol>\n",
    "<li>Connect to the Big Data infrastructure and download data from github;\n",
    "<li>Execute a WordCount MapReduce job;\n",
    "<li>Create and execute a LineCount MapReduce job;\n",
    "<li>Create and execute a Map Reduce job with a Combiner;\n",
    "<li>Create and execute a Map Reduce job to search for specific data;\n",
    "<li>Create and execute a Map Reduce job to join two datasets.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Hadoop cluster from a windows PC, follow these instructions:\n",
    "<ol>\n",
    "<li>Start Putty (type putty into the search bar on the start menu).</li>\n",
    "<li>In the hostname textbox, type the HOSTNAME field provided on the printout, ensuring that Connection type is set to ssh.</li>\n",
    "<li>Click Open.</li>\n",
    "<li>On first connection, you will be asked a question about connection security. Please click \"yes\".\n",
    "<li>You will be asked for your username and password.</li>\n",
    "<li>Usernames are trainingN (with N replaced by your allocated number) and password, provided on the printout.</li>\n",
    "</ol>\n",
    "\n",
    "To connect to the Hadoop cluster from a Mac, follow these instructions:\n",
    "<ol>\n",
    "<li>Start Terminal (type terminal into the spotlight search bar).</li>\n",
    "<li>Type the following command, followed by enter:\n",
    "<br>ssh USERNAME@HOSTNAME </li>\n",
    "<li>On first connection, you will be asked a question about connection security. Please type \"yes\".\n",
    "<li>You will be asked for your username and password.</li>\n",
    "<li>Usernames are trainingN (with N replaced by your allocated number) and password, provided on the printout.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is available from the command line via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this command will display a help message on the console. Please take a look at the printed help, as you may wish to consult such help during the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to download all of the supporting material for this lab. This will be done using git. If you would like to learn more about git, please consult [Git basics](https://git-scm.com/book/en/v2/Getting-Started-Git-Basics). However, the instructions within this document (and subsequent documents) should be sufficient for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type the following command to download the course material:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/markbriers/bd-sp-2017.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a short delay (whilst the material is downloaded), you should be able to change your working directory to bd-sp-2017 via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd bd-sp-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second command will list the contents of the folder. There are four directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now connected to the Hadoop cluster, have taken a first look at the available Hadoop commands, and have cloned the git repository that we will use throughout the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to interact with the Hadoop Distributed Filesystem (HDFS). HDFS help can be obtained by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now list the contents of the data directory to be used on this course, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /rss-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing this command, you should see a list of five data files listed in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create a new folder in HDFS (via -mkdir), and move (-mv) the heathrowdata.txt file into this new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /rss-data/temperatureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mv /rss-data/heathrowdata.txt /rss-data/temperatureData/heathrowdata.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following activities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Move the wickairportdata.txt file into the /rss-data/temperatureData folder.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>List the contents of the /rss-data/temperatureData/ folder.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now be executing a Map Reduce job that performs word count. First, change your working directory to exercise 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd exercise3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that there are two files in this folder; a mapper and a reducer. You will need to update the properties of these files, so that they are appropriately formatted, and have execute permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perl -pi -e 's/\\r\\n/\\n/g' mapper.py\n",
    "perl -pi -e 's/\\r\\n/\\n/g' reducer.py\n",
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two commands modify the line endings, to ensure that they are compatible with Hadoop streaming code. The third command modifies the permissions on all python files (with .py extension) so that they can be executed via the bash shell. <b>Note that these commands will need to be executed for all exercises, but will not be listed again.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the mapper.py and reducer.py files using your favourite Linux text editor (instructions for vim can be found [here](https://www.engadget.com/2012/07/10/vim-how-to/); instructions for nano can be found [here](https://www.howtogeek.com/howto/42980/the-beginners-guide-to-nano-the-linux-command-line-text-editor/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Check that you understand the contents of each Python file.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to execute a Hadoop job across the sample.txt data file, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar\n",
    "-files mapper.py,reducer.py\n",
    "-mapper mapper.py\n",
    "-reducer reducer.py\n",
    "-input /rss-data/sample.txt\n",
    "-output /rss-results/wordcountout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After execution, results will be output to the /rss-results/wordcountout/ folder in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>List the contents of the wordcountout folder.<br>\n",
    "How many part files are produced?<br>\n",
    "What does this number indicate about the number of reducer processes that were executed?<br>\n",
    "What should we have done to improve the output? [HINT: Look at the keyspace and punctuation.]</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to view the contents of the results is to cat the files into the Linux less paging utility function, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -text /rss-results/wordcountout/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your space bar to page through the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change your working directory to exercise4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../exercise4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Using the stub mapper.py and reducer.py functions in the w1e4 working directory, write and execute a Map Reduce program that:\n",
    "<ol><li>computes the length of each line during the Map phase, outputting (lineLength, 1) as the (key, value) pair\n",
    "counts the number of occurrences of lineLength during the reduce phase, outputting (lineLength, count) as (key, value) pairs.<li>View the results using the cat and less utility functions.</ol><br>What do you notice about the results?</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Change (cd) to the exercise5 directory and list (ll) the files in this directory.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see, the folder contains three python files; mapper.py, reducer.py and combiner.py. You will now execute a distributed word count map reduce job that uses a combiner to reduce the amount of network traffic prior to the shuffle and sort phase of the Map Reduce process. This is executed via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar\n",
    "-files mapper.py,reducer.py,combiner.py\n",
    "-mapper mapper.py\n",
    "-reducer reducer.py\n",
    "-combiner combiner.py\n",
    "-input /rss-data/sample.txt\n",
    "-output /rss-results/wordcountout-combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><i>\n",
    "<li>Change (cd) to the exercise6 directory.\n",
    "<li> Using the pre-populated mapper.py and reducer.py functions, write a combiner function (using combiner.py) that improves the reducer efficiency of the Map Reduce program. This map reduce process should be executed across the files in the textData directory in HDFS.</i></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><i><li>Change (cd) to the exercise7 directory.</i></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperatureData folder on HDFS contains two text files that are populated with monthly meteorological data from Heathrow airport (London) and Wick airport (Scotland) between 1948 to 2015. The columns within each data file correspond to the following headers:<br><br>\n",
    "<i>Year ~ Month ~ MaxTemp ~ MinTemp ~ Rainfall</i><br><br>\n",
    "A typical SQL operation performed within a relational database is to join two (or more) tables, using a specified join key (this is the same as the merge function in R). In the case of the temperature data, one could join the data on Year-Month, listing the Heathrow airport values and then the Wick airport values on the same line. That is, a line from the joined data (by Year-Month) would for formatted as follows:<br><br>\n",
    "<i>Year ~ Month ~ HMaxTemp ~ HMinTemp ~ HRainfall ~ WMaxTemp ~ WMinTemp ~ WRainfall</i><br><br>\n",
    "where the H prefix corresponds to data from Heathrow airport, and the W prefix corresponds to data from Wick airport, for the same Year and Month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><i><li>Write a Map Reduce program that joins the two temperature datasets (/rss-data/temperatureData/heathrowdata.txt and /rss-data/temperatureData/wickairportdata.txt in HDFS) using Year-Month as the join key. The output should be the consistent with the joined line defined directly above.</i></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><i><li> Change (cd) to the exercise7 directory.\n",
    "<li>Write a Map Reduce program that outputs data for September only using the joined results from Exercise 3. Get your results from HDFS and place them in the local filesystem. Using R or matplotlib in Python (R is available by typing R at the command line prompt and/or Python is available by typing python at the command line prompt) plot two time-series of maximum temperature data, sorted by year, for each location (Heathrow airport and Wick airport). What do you notice about the results?</i></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><i><li> Change (cd) to the exercise8 directory.\n",
    "<li>Write a Map Reduce program to compute the parameters of an ordinary least squares model. Apply the programme to estimate \"maximum temperature\" data, where y = Heathrow airport data and x = Wick airport data.<i></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
